{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98254f7-d2d3-452b-9e49-9547a4955c99",
   "metadata": {},
   "source": [
    "# Code Analysis Demo with LLMs\n",
    "\n",
    "This notebook demonstrates the basic usage of the LLM Code Analyzer system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628e23b-1807-40ef-a31a-905129016679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_analyzer import CodeAnalyzer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CodeAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323e811-a048-44ef-8180-d5de401e686d",
   "metadata": {},
   "source": [
    "## Analyzing a Simple Function\r\n",
    "Let's analyze a simple Python function using different LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38043694-b90e-4731-b0fa-3efc5348dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code to analyze\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "# Analyze with GPT\n",
    "gpt_result = analyzer.analyze_code(sample_code, model=\"gpt\")\n",
    "print(\"GPT Analysis Results:\")\n",
    "print(analyzer.generate_report(gpt_result))\n",
    "\n",
    "# Analyze with Claude\n",
    "claude_result = analyzer.analyze_code(sample_code, model=\"claude\")\n",
    "print(\"\\nClaude Analysis Results:\")\n",
    "print(analyzer.generate_report(claude_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791c6c7-81dd-426f-94ad-a8c86f7d4b0f",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ffbc6-eaaa-403c-a9b3-810334cb8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model comparison\n",
    "comparison = analyzer.get_model_comparison()\n",
    "\n",
    "# Print comparison results\n",
    "for model_name, eval_result in comparison.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Average Quality Score: {eval_result.average_quality_score:.2f}\")\n",
    "    print(f\"Average Execution Time: {eval_result.average_execution_time:.2f}s\")\n",
    "    print(f\"Success Rate: {eval_result.success_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c431dd7-7886-4c10-b5c4-98eaa2289e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
